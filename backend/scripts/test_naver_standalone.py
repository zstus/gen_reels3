#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìŠ¤í¬ë˜í•‘ ë…ë¦½ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import requests
import time
from bs4 import BeautifulSoup
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def scrape_naver_blog(url: str) -> str:
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì „ìš© ìŠ¤í¬ë˜í•‘ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    max_retries = 3
    retry_delay = 2.0

    # ê³ ê¸‰ í—¤ë” ì„¤ì • (ì‹¤ì œ ë¸Œë¼ìš°ì € ëª¨ë°©)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0'
    }

    # ì„¸ì…˜ ìƒì„± ë° ë‹¨ê³„ë³„ ì ‘ê·¼
    session = requests.Session()

    for attempt in range(max_retries):
        try:
            logger.info(f"ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìŠ¤í¬ë˜í•‘ ì‹œë„ {attempt + 1}/{max_retries}: {url}")

            # 1ë‹¨ê³„: ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ (ì¿ í‚¤ ì„¤ì •)
            logger.info("1ë‹¨ê³„: ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸")
            session.get('https://www.naver.com', headers=headers, timeout=10)

            # 2ë‹¨ê³„: ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë©”ì¸ ë°©ë¬¸
            logger.info("2ë‹¨ê³„: ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë©”ì¸ ë°©ë¬¸")
            session.get('https://blog.naver.com', headers=headers, timeout=10)
            time.sleep(1.0)  # ìì—°ìŠ¤ëŸ¬ìš´ ë¸Œë¼ìš°ì§• íŒ¨í„´

            # 3ë‹¨ê³„: ì‹¤ì œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì ‘ê·¼
            logger.info("3ë‹¨ê³„: ëŒ€ìƒ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì ‘ê·¼")

            # Referer í—¤ë” ì¶”ê°€ (ë„¤ì´ë²„ ë¸”ë¡œê·¸ì—ì„œ ì˜¨ ê²ƒì²˜ëŸ¼)
            blog_headers = headers.copy()
            blog_headers['Referer'] = 'https://blog.naver.com'

            response = session.get(url, headers=blog_headers, timeout=15)
            response.raise_for_status()

            logger.info(f"âœ… HTTP ìš”ì²­ ì„±ê³µ: {response.status_code}")
            logger.info(f"ì‘ë‹µ í¬ê¸°: {len(response.content)} bytes")
            logger.info(f"Content-Type: {response.headers.get('Content-Type', 'N/A')}")

            # HTML íŒŒì‹±
            soup = BeautifulSoup(response.text, 'html.parser')

            # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì „ìš© ì½˜í…ì¸  ì„ íƒìë“¤
            naver_selectors = [
                '.se-main-container',  # ìŠ¤ë§ˆíŠ¸ì—ë””í„° 3.0
                '.se-component-content',  # ìŠ¤ë§ˆíŠ¸ì—ë””í„° ì»´í¬ë„ŒíŠ¸
                '#postViewArea',  # êµ¬ë²„ì „ ë¸”ë¡œê·¸
                '.post-view',     # ì¼ë°˜ ë¸”ë¡œê·¸ ë·°
                '.blog-content',  # ë¸”ë¡œê·¸ ì½˜í…ì¸  ì˜ì—­
                '#content',       # ì¼ë°˜ ì½˜í…ì¸ 
                '.contents_style'  # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì½˜í…ì¸  ìŠ¤íƒ€ì¼
            ]

            content_text = ""

            # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì „ìš© ì„ íƒìë¡œ ì½˜í…ì¸  ì¶”ì¶œ
            for selector in naver_selectors:
                elements = soup.select(selector)
                if elements:
                    logger.info(f"âœ… ì½˜í…ì¸  ì„ íƒì ë§¤ì¹­: {selector}")
                    for elem in elements:
                        # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
                        for script in elem(["script", "style"]):
                            script.decompose()
                        content_text += elem.get_text(strip=True, separator='\n') + "\n"
                    break

            # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì½˜í…ì¸ ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if not content_text.strip():
                logger.warning("âŒ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì „ìš© ì„ íƒìë¡œ ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                logger.info("ì „ì²´ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„")

                # ë¶ˆí•„ìš”í•œ íƒœê·¸ë“¤ ì œê±°
                for unwanted in soup(["script", "style", "nav", "header", "footer", "aside"]):
                    unwanted.decompose()

                content_text = soup.get_text(strip=True, separator='\n')

            # í…ìŠ¤íŠ¸ ì •ë¦¬
            lines = [line.strip() for line in content_text.split('\n') if line.strip()]
            clean_text = '\n'.join(lines)

            # Debug: Show what we actually extracted
            logger.info(f"Debug: Extracted content preview: '{clean_text[:200]}'")

            if len(clean_text) < 100:
                logger.warning(f"WARNING: Extracted text too short: {len(clean_text)} chars")
                logger.info(f"Full extracted content: '{clean_text}'")

                # Also check raw HTML for debugging
                logger.info(f"Raw HTML preview (first 1000 chars): {response.text[:1000]}")

                if attempt < max_retries - 1:
                    logger.info(f"Retrying in {retry_delay} seconds...")
                    time.sleep(retry_delay)
                    retry_delay *= 1.5  # Exponential backoff
                    continue

            logger.info(f"SUCCESS: Content extraction complete: {len(clean_text)} chars")

            # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì•ë¶€ë¶„ë§Œ ì‚¬ìš© (GPT í† í° ì œí•œ ê³ ë ¤)
            max_chars = 8000
            if len(clean_text) > max_chars:
                clean_text = clean_text[:max_chars]
                logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ: {max_chars}ìë¡œ ì˜ë¼ëƒ„")

            return clean_text

        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code if e.response else 'unknown'
            logger.error(f"âŒ HTTP ì—ëŸ¬ (ì‹œë„ {attempt + 1}): {status_code}")

            if status_code == 403:
                logger.warning("âš ï¸ ì ‘ê·¼ ê¸ˆì§€ (403) - IP ì°¨ë‹¨ ë˜ëŠ” ë´‡ ê°ì§€ ê°€ëŠ¥ì„±")
            elif status_code == 400:
                logger.warning("âš ï¸ ì˜ëª»ëœ ìš”ì²­ (400) - í—¤ë” ë˜ëŠ” ìš”ì²­ í˜•ì‹ ë¬¸ì œ")

            if attempt < max_retries - 1:
                logger.info(f"ì¬ì‹œë„ ëŒ€ê¸°: {retry_delay}ì´ˆ")
                time.sleep(retry_delay)
                retry_delay *= 1.5
            else:
                raise

        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ ìš”ì²­ ì—ëŸ¬ (ì‹œë„ {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(retry_delay)
                retry_delay *= 1.5
            else:
                raise
        except Exception as e:
            logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ (ì‹œë„ {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(retry_delay)
                retry_delay *= 1.5
            else:
                raise

def test_naver_blog_extraction():
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""

    test_url = "https://blog.naver.com/ilovekwater/221658073924"

    print("=" * 70)
    print("Naver Blog Scraping Test")
    print("=" * 70)
    print(f"Target URL: {test_url}")
    print()

    try:
        result = scrape_naver_blog(test_url)

        if result and len(result) > 100:
            print("SUCCESS: Scraping completed!")
            print(f"Extracted text length: {len(result)} characters")
            print()
            print("Content preview (first 500 chars):")
            print("-" * 50)
            preview = result[:500] if len(result) > 500 else result
            print(preview)
            if len(result) > 500:
                print("... (truncated)")
            print("-" * 50)

            return True
        else:
            print("FAILED: Content is empty or too short")
            return False

    except Exception as e:
        print(f"FAILED: Scraping error: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_naver_blog_extraction()

    print("\n" + "=" * 50)
    print("Test Results")
    print("=" * 50)

    if success:
        print("SUCCESS: Naver blog 400 error has been resolved!")
        print("The enhanced scraping implementation is working:")
        print("   - Advanced browser header simulation")
        print("   - Session-based step-by-step access")
        print("   - Retry logic with exponential backoff")
        print("   - Naver blog-specific content selectors")
    else:
        print("FAILED: Still encountering issues.")
        print("Additional improvement suggestions:")
        print("   - Longer wait times")
        print("   - Different User-Agent attempts")
        print("   - Proxy server usage")
        print("   - Consider Selenium browser automation")